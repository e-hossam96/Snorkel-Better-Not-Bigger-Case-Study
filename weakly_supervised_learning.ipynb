{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff6fb58",
   "metadata": {},
   "source": [
    "# Weakly Learning on IMDB Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1856958",
   "metadata": {},
   "source": [
    "Learning using BERT base uncased over the weak labels of the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23aed5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5208102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ccf65",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ab25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('./train.json')\n",
    "temp_df = pd.read_json('./valid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c8fca7-965e-4250-9408-ce5054fd0c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>weak_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George P. Cosmatos' \"Rambo: First Blood Part I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>While this movie's style isn't as understated ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really awful... lead actor did OK... the film,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Home Room deals with a Columbine-like high-sch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  weak_labels\n",
       "1  This movie is a great. The plot is very true t...      1            1\n",
       "2  George P. Cosmatos' \"Rambo: First Blood Part I...      0            1\n",
       "5  While this movie's style isn't as understated ...      1            0\n",
       "7  really awful... lead actor did OK... the film,...      0            0\n",
       "9  Home Room deals with a Columbine-like high-sch...      1            1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f973b5-33c5-4887-a31c-662837e32f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12506 entries, 1 to 26542\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         12506 non-null  object\n",
      " 1   label        12506 non-null  int64 \n",
      " 2   weak_labels  12506 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892b987e-b959-44bf-b00d-000f29107759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13053 entries, 0 to 28706\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         13053 non-null  object\n",
      " 1   label        13053 non-null  int64 \n",
      " 2   weak_labels  13053 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 407.9+ KB\n"
     ]
    }
   ],
   "source": [
    "temp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb96e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = temp_df.iloc[:10000, :].reset_index(drop=True)\n",
    "test_df = temp_df.iloc[10000:, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b6e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['text', 'weak_labels']], split='train')\n",
    "valid_ds = Dataset.from_pandas(valid_df[['text', 'weak_labels']], split='valid')\n",
    "test_ds = Dataset.from_pandas(test_df[['text', 'label']], split='test')\n",
    "\n",
    "ds = DatasetDict({'train': train_ds, 'valid': valid_ds, 'test': test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c604f8-5e6c-4abc-a34f-15b5ec428b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'] = ds['train'].remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1026e602-7fd7-4214-b8a9-9444e9297f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'] = ds['train'].rename_column('weak_labels', 'label')\n",
    "ds['valid'] = ds['valid'].rename_column('weak_labels', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54850e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3053\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593cb1a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f8d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aa82a245484b95989ef3857a9c0471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e7e9a1699f434799d2a31d5838becd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f8f8b3f42a4f8480e082b19c7d45b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a28fa90332c4e0ebb462b56698ce2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d0c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ds(field):\n",
    "    '''Tokenize examples from dataset.'''\n",
    "    return tokenizer(field['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3541c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b1fc7ae33748b998ce271384866b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38115c749c2d46e588275103ca0158a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85a7bbba00d497abf104176aa1e20c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_encoded = ds.map(tokenize_ds, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f93031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12506\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3053\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0a042",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df55bb1f-a2d5-4784-ba2c-ec931d62df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c65b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_scores(preds):\n",
    "    '''Compute scores of transformers predictions.'''\n",
    "    logits, labels = preds\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    f1 = f1_score(labels, pred)\n",
    "    return {'Accuracy': acc, 'F1 Score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25170292",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'negative', 1: 'positive'}\n",
    "label2id = {'negative': 0, 'positive': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1004bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c260dcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04596d9ea6a413f87ac496a012fa619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=2, \n",
    "                          id2label=id2label, label2id=label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "207190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./bert-base-uncased-imdb\", \n",
    "                                  learning_rate=2e-5, \n",
    "                                  per_device_train_batch_size=16, \n",
    "                                  per_device_eval_batch_size=16, \n",
    "                                  num_train_epochs=2, \n",
    "                                  weight_decay=0.01, \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_strategy=\"epoch\", \n",
    "                                  load_best_model_at_end=True, \n",
    "                                  logging_steps=500, \n",
    "                                  log_level=\"error\", \n",
    "                                  push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3735f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=ds_encoded['train'], \n",
    "                  eval_dataset=ds_encoded['valid'], \n",
    "                  tokenizer=tokenizer, \n",
    "                  data_collator=data_collator, \n",
    "                  compute_metrics=compute_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dfb62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/snorkel-better-not-bigger/wandb/run-20230423_215527-1r25iko9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/e_hossam96/huggingface/runs/1r25iko9\" target=\"_blank\">./bert-base-uncased-imdb</a></strong> to <a href=\"https://wandb.ai/e_hossam96/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1564' max='1564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1564/1564 23:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.331450</td>\n",
       "      <td>0.868700</td>\n",
       "      <td>0.896361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.385913</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.894247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1564, training_loss=0.27681667054705605, metrics={'train_runtime': 1435.493, 'train_samples_per_second': 17.424, 'train_steps_per_second': 1.09, 'total_flos': 6314628328647840.0, 'train_loss': 0.27681667054705605, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d830e8f1-0e19-404d-a5e4-8f258b9e2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.28623396158218384,\n",
       " 'test_Accuracy': 0.8869963969865706,\n",
       " 'test_F1 Score': 0.9127686472819218,\n",
       " 'test_runtime': 45.7788,\n",
       " 'test_samples_per_second': 66.69,\n",
       " 'test_steps_per_second': 4.172}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model\n",
    "\n",
    "test_outs = trainer.predict(ds_encoded['test'])\n",
    "\n",
    "test_outs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a2657e8-12a3-48f6-b943-339810c618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(test_outs[0], axis=-1)\n",
    "labels = test_outs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44a26546-5284-4b29-b13e-5bfb59c24632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84      1179\n",
      "           1       0.87      0.96      0.91      1874\n",
      "\n",
      "    accuracy                           0.89      3053\n",
      "   macro avg       0.90      0.86      0.88      3053\n",
      "weighted avg       0.89      0.89      0.88      3053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
